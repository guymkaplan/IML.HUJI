{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Oe/hrhj6QQWVgMuPGRhj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guymkaplan/IML.HUJI/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2gpDIW7wC7C",
        "outputId": "f016f4c9-fb56-4558-cc01-ef9fccb99e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUHym_hpwcPJ",
        "outputId": "c6c59047-8864-4929-e4ef-885b130f71e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# subset of categories that we will use\n",
        "category_dict = {'comp.graphics': 'computer graphics',\n",
        "                 'rec.sport.baseball': 'baseball',\n",
        "                 'sci.electronics': 'science, electronics',\n",
        "                 'talk.politics.guns': 'politics, guns'\n",
        "                 }"
      ],
      "metadata": {
        "id": "Brwr8EDU2Jtg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(categories=None, portion=1.):\n",
        "    \"\"\"\n",
        "    Get data for given categories and portion\n",
        "    :param portion: portion of the data to use\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # get data\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    data_train = fetch_20newsgroups(categories=categories, subset='train', remove=('headers', 'footers', 'quotes'),\n",
        "                                    random_state=21)\n",
        "    data_test = fetch_20newsgroups(categories=categories, subset='test', remove=('headers', 'footers', 'quotes'),\n",
        "                                   random_state=21)\n",
        "\n",
        "    # train\n",
        "    train_len = int(portion*len(data_train.data))\n",
        "    x_train = np.array(data_train.data[:train_len])\n",
        "    y_train = data_train.target[:train_len]\n",
        "    # remove empty entries\n",
        "    non_empty = x_train != \"\"\n",
        "    x_train, y_train = x_train[non_empty].tolist(), y_train[non_empty].tolist()\n",
        "\n",
        "    # test\n",
        "    x_test = np.array(data_test.data)\n",
        "    y_test = data_test.target\n",
        "    non_empty = np.array(x_test) != \"\"\n",
        "    x_test, y_test = x_test[non_empty].tolist(), y_test[non_empty].tolist()\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "O_v_gu8BxKsx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "def linear_classification(portion=1.):\n",
        "    \"\"\"\n",
        "    Perform linear classification\n",
        "    :param portion: portion of the data to use\n",
        "    :return: classification accuracy\n",
        "    \"\"\"\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    tf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    x_train, y_train, x_test, y_test = get_data(categories=category_dict.keys(), portion=portion)\n",
        "    X = tf.fit_transform(x_train)\n",
        "    X_test = tf.fit_transform(x_test)\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(X, y=y_train)\n",
        "    \n",
        "    return accuracy_score(y_test, classifier.predict(X_test))"
      ],
      "metadata": {
        "id": "-XtMGT4q1YNH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    portions = [0.1, 0.5, 1.]\n",
        "    # Q1\n",
        "    print(\"Logistic regression results:\")\n",
        "    _accuracy = []\n",
        "    for p in portions:\n",
        "        print(f\"Portion: {p}\")\n",
        "        result = linear_classification(p)\n",
        "        _accuracy.append(result)\n",
        "        print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CMaIAgY2BDJ",
        "outputId": "9be05e7c-cb5b-4195-baa2-030482e9768b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression results:\n",
            "Portion: 0.1\n",
            "0.27320954907161804\n",
            "Portion: 0.5\n",
            "0.3090185676392573\n",
            "Portion: 1.0\n",
            "0.3295755968169761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(portions,_accuracy, marker='o', linestyle='dashed')\n",
        "plt.xlabel(\"Portion\"), plt.ylabel(\"Accuracy\"), plt.title(\n",
        "        \"Logistic Regression Accuracy achieved as a function of portion of data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_XOWPFQj2CJE",
        "outputId": "048d32ab-4695-4431-beb9-4505dbbe0903"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "def transformer_classification(portion=1.):\n",
        "    \"\"\"\n",
        "    Transformer fine-tuning.\n",
        "    :param portion: portion of the data to use\n",
        "    :return: classification accuracy\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    class Dataset(torch.utils.data.Dataset):\n",
        "        \"\"\"\n",
        "        Dataset object\n",
        "        \"\"\"\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    from datasets import load_metric\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    from transformers import Trainer, TrainingArguments\n",
        "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', cache_dir=None)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base',\n",
        "                                                               cache_dir=None,\n",
        "                                                               num_labels=len(category_dict),\n",
        "                                                               problem_type=\"single_label_classification\")\n",
        "\n",
        "    x_train, y_train, x_test, y_test = get_data(categories=category_dict.keys(), portion=portion)\n",
        "    training_args = TrainingArguments(output_dir=\"/ex5\",learning_rate=5e-5, per_device_train_batch_size=1, per_device_eval_batch_size=1, num_train_epochs=1)\n",
        "    dataset = Dataset(tokenizer(x_train,padding=True,truncation=True), y_train)\n",
        "    dataset_test = Dataset(tokenizer(x_test,padding=True,truncation=True), y_test)\n",
        "\n",
        "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset,eval_dataset=dataset_test, tokenizer=tokenizer)\n",
        "    trainer.train()\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "r66bdxrM2__5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"\\nFinetuning results:\")\n",
        "    _accuracy = []\n",
        "\n",
        "    for p in portions:\n",
        "        print(f\"Portion: {p}\")\n",
        "        result = transformer_classification(portion=p)\n",
        "        _accuracy.append(result)\n",
        "        print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "90v9qGV47BTA",
        "outputId": "860e0fc1-52a4-4597-f224-37ef3445e0bd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finetuning results:\n",
            "Portion: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 229\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 229\n",
            "  Number of trainable parameters = 82121476\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='81' max='229' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 81/229 05:10 < 09:41, 0.25 it/s, Epoch 0.35/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-6b23bc77bc19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mportions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Portion: {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-8f097df4cfaa>\u001b[0m in \u001b[0;36mtransformer_classification\u001b[0;34m(portion)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         )\n\u001b[0;32m-> 1527\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         )\n\u001b[0;32m--> 853\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 )\n\u001b[1;32m    526\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    528\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 339\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classification(portion=1.):\n",
        "    \"\"\"\n",
        "    Perform zero-shot classification\n",
        "    :param portion: portion of the data to use\n",
        "    :return: classification accuracy\n",
        "    \"\"\"\n",
        "    from transformers import pipeline\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    import torch\n",
        "    x_train, y_train, x_test, y_test = get_data(categories=category_dict.keys(), portion=portion)\n",
        "    clf = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-MiniLM2-L6-H768')\n",
        "    candidate_labels = list(category_dict.values())\n",
        "    # clf.(x_train, y_train, candidate_labels)\n",
        "\n",
        "    y_hat = clf.predict(x_test)\n",
        "    # Add your code here\n",
        "    # see https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline\n",
        "    return accuracy_score(y_hat,y_test)"
      ],
      "metadata": {
        "id": "KyTt1z6I7O0x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"\\nZero-shot result:\")\n",
        "    print(zeroshot_classification())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8JAL2YggPEhj",
        "outputId": "5d032e98-fd92-4960-de67-ba4fcc61c676"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-shot result:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9d313848a878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nZero-shot result:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeroshot_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-0bca744960a3>\u001b[0m in \u001b[0;36mzeroshot_classification\u001b[0;34m(portion)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# clf.(x_train, y_train, candidate_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Add your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# see https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mScikit\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0minterface\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mto\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \"\"\"\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to understand extra arguments {args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 )\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 )\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Try to return next item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubiterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# When a preprocess iterator ends, we can start lookig at the next item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, inputs, candidate_labels, hypothesis_template)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0msequence_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcandidate_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, labels, hypothesis_template)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must include at least one label and at least one sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QaLn0FN0PL33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}